<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hung-Chieh Fang</title>

    <meta name="author" content="Hung-Chieh Fang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <style>
      a {
        color: #1460a7; /* soft gray */
        text-decoration: none;
      }
  
      a:hover {
        color: #1460a7; /* black on hover */
        text-decoration: underline;
      }
    </style>
    
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hung-Chieh Fang
                </p>
                <p>I am a senior undergrad majoring in Computer Science at National Taiwan University, where I am fortunate to be advised by Professors <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>, <a href="https://shaohua0116.github.io/">Shao-Hua Sun</a> and <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung (Vivian) Chen</a>. </p>
                <p>Currently, I am a visiting research intern at <a href="https://www.stanford.edu/">Stanford University</a>, working with Prof. <a href="https://dorsa.fyi/">Dorsa Sadigh</a>.
                <p>Previously, I was a visiting student at <a href="https://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>, where I had the privilege of working with Dr. <a href="https://yifeiacc.github.io/">Yifei Zhang</a> and Prof. <a href="https://www.cse.cuhk.edu.hk/people/faculty/irwin-king/">Irwin King</a>.</p>
                <p style="color: rgb(255, 3, 45);">I am applying for PhD programs in Fall 2026.</p>
                <p style="text-align:center">
                  <a href="mailto:hungchieh.fang@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/cv_hcfang.pdf">CV</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=MpGIrR0AAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/hc-fang/">Github</a> &nbsp;/&nbsp;
                  <a href="https://x.com/hungchiehfang">X</a> 
                  <!-- <a href="https://www.linkedin.com/in/HUNG-CHIEH-FANG/">Linkedin</a>  -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/hcfang.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/hcfang.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="margin-top: -20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr><td style="padding:0px 20px 10px 20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <p></p>
            </td></tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-top: -20px;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <tr>  
              <td style="padding: 10px 20px 5px 20px; width:70%; vertical-align:middle;">
                <strong><span class="newsdate">- [June 2025] </span></strong>
                Our paper on <a href="https://ssd-uniformity.github.io/">improving generalization under non-IID data</a> has been accepted to ICCV 2025. See you in Hawaii!
              </td>
            </tr>
            <tr>  
              <td style="padding: 10px 20px 5px 20px; width:70%; vertical-align:middle;">
                <strong><span class="newsdate">- [June 2025] </span></strong>
                I'm fortunate to visit <a href="https://iliad.stanford.edu/">ILIAD</a> at Stanford, hosted by Prof. <a href="https://dorsa.fyi/">Dorsa Sadigh</a>.
              </td>
            </tr>
            <tr>  
              <td style="padding: 10px 20px 5px 20px; width:70%; vertical-align:middle;">
                <strong><span class="newsdate">- [May 2025] </span></strong>
                Our paper on <a href="https://dc-unida.github.io/">universal domain adaptation across any class priors</a> has been accepted to ICML 2025. See you in Vancouver!
              </td>
            </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px 20px 20px 20px; width:100%;vertical-align:middle; line-height: 1.4">
                <h2>Research</h2>
                <p>
                  I am broadly interested in robotics, representation learning and reinforcement learning. My goal is to develop robots with <i>generalist intelligence</i> that can <i>autonomously adapt</i> through interaction and feedback from both humans and the physical world. 
                  <!-- I am particularly interested in: -->
                  <!-- <ul>
                    <li style="margin-bottom: 0.5rem;">
                    <strong>Representation learning for generalist policies</strong>: (1) learning <a href="https://ssd-uniformity.github.io/">expressive representations</a> (2) discovering the <a href="data/sof.pdf">underlying shared structure</a> across different modalities (e.g., vision, text, control, touch).
                    </li>

                    <li style="margin-bottom: 0.5rem;">
                      <strong>Representation learning for adaptation</strong>: (1) enabling robust <a href="https://dc-unida.github.io/">adaptation to distribution shift</a> (2) developing unsupervised reinforcement learning algorithms for rapid online exploration.
                    </li>

                    <li style="margin-bottom: -1.5rem;">
                      <strong>Dexterous manipulation</strong>: scaling up dexterous hand capabilities across diverse tasks by learning from diverse sources (e.g., simulation, human videos).
                    </li>
                  </ul> -->
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <!-- <td style="padding:20px 0px 0px 20px;width:25%;vertical-align:middle"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/arm_finger_clip.gif' width="160">
              </div>
            </td>
            <!-- <td style="padding:0px 0px 0px 0px;width:75%;vertical-align:middle"> -->
            <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">DexDrummer: In-Hand, Contact-Rich, and Long-Horizon Dexterous Robot Drumming</span>
                <br>
              <strong>Hung-Chieh Fang</strong>, <a href="https://amberxie88.github.io/">Amber Xie</a>, <a href="https://jenngrannen.com/">Jennifer Grannen</a>, <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
               <br>
              Under Review 
                <br>
              <a href="data/dexdrummer.pdf">paper</a> / <a href="https://sites.google.com/view/dexdrummer/">website</a> 
                <div style="margin-top:5px">
                We propose <i>drumming</i> as a unified testbed for in-hand, contact-rich, and long-horizon dexterous manipulation. We present DexDrummer, a general <i>contact-targeted</i> RL recipe that enables dexterous skills for long horizon songs.
                </div>
              <!-- </div> -->
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/sof.jpg' width="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Learning Skill Abstraction from Action-Free Videos</span>
              <br>
              <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung-906.github.io/">Kuo-Han Hung</a>*, <a href="https://openreview.net/profile?id=~Chu-Rong_Chen1">Chu-Rong Chen</a>, <a href="https://openreview.net/profile?id=~Po-Jung_Chou1">Po-Jung Chou</a>, <a href="https://yck1130.github.io/">Chun-Kai Yang</a>, <a href="https://pochen-ko.github.io/">Po-Chen Ko</a>, <a href="https://vllab.ee.ntu.edu.tw/ycwang.html">Yu-Chiang Wang</a>, <a href="https://kristery.github.io/">Yueh-Hua Wu</a>, <a href="https://minhungchen.netlify.app/">Min-Hung Chen</a>, <a href="https://shaohua0116.github.io/">Shao-Hua Sun</a>
              <!-- <div style="margin-top:2px">
              <em>Under Review in ICLR</em>
              </div> -->
              <br>
              <em>ICML Workshop on Building Physically Plausible World Models, 2025</em>
              <br>
              <a href="data/sof.pdf">paper</a> / <a href="https://sof-video.github.io/">website</a> / <a href="data/sof_poster.pdf">poster</a>
              <div style="margin-top:5px">
                We propose SOF, a method that leverages temporal structures in videos while enabling easier translation to low-level control. SOF learns a latent <i>skill</i> space through <i>optical flow representations</i> that better aligns video and action dynamics, thereby improving long-horizon performance.
              </div>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ssd.png' width="190">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</span>
                <br>
                <strong>Hung-Chieh Fang</strong>, <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>, <a href="https://www.cse.cuhk.edu.hk/people/faculty/irwin-king/">Irwin King</a>, <a href="https://yifeiacc.github.io/">Yifei Zhang</a> 
                <br>
                <em>International Conference on Computer Vision (ICCV), 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2508.01251">paper</a> / <a href="https://ssd-uniformity.github.io/">website</a> / <a href="https://github.com/hc-fang/ssd">code</a> / <a href="data/ssd_iccv25_poster.pdf">poster</a>
                <div style="margin-top:5px">
                  We explore how to improve generalization under highly non-IID data distributions where representations are non-shared. We propose a plug-and-play regularizer that encourages dispersion to improve uniformity without sacrificing semantic alignment.
                </div>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/unida_thumbnail.png' width="190">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation</span>
                <br>
                <strong>Hung-Chieh Fang</strong>, <a href="https://ariapoy.github.io/">Po-Yi Lu</a>, <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>
                <br>
                <em>International Conference on Machine Learning (ICML), 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2410.11271">paper</a> / <a href="https://dc-unida.github.io/">website</a> / <a href="data/unida_icml25_poster.pdf">poster</a>
                <div style="margin-top:5px">
                  We study how to adapt to arbitrary target domains <i>without assuming any class-set priors</i>. Existing methods suffer from severe negative transfer under large class-set shifts due to the overestimation of importance weights. We propose a simple uniformity loss that increases the entropy of target representations and improves performance across all class-set priors.
                </div>
              </td>
            </tr>

           
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/convadrqa.png' width="190">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Open-domain Conversational Question Answering with Historical Answers</span>
                <br>
                <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung-906.github.io/">Kuo-Han Hung</a>*, <a href="https://chaoweihuang.github.io/">Chao-Wei Huang</a>, <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung Chen</a>
                <br>
                <em>Asian Chapter of the Association for Computational Linguistics (AACL), 2022</em>
                <br>
                <a href="https://arxiv.org/abs/2211.09401">paper</a> / <a href="https://github.com/MiuLab/ConvADR-QA">code</a>
                <div style="margin-top:5px">
                  We propose combining the signal from historical answers with the <em>noise-reduction</em> ability of knowledge distillation to improve information retrieval and question answering.
                </div>
              </td>
            </tr>
          </tbody></table>
          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Course Projects</h2>
            </td>
          </tr>
        </tbody></table> -->
        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:25px 0px 0px 0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/pwhubert.png' width="220">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model</span>
              <div style="margin-top:4px">
              <strong>Hung-Chieh Fang*</strong>, <a href="https://openreview.net/profile?id=~Nai-Xuan_Ye1">Nai-Xuan Ye*</a>, <a href="https://www.cs.utexas.edu/~yjshih/">Yi-Jen Shih</a>, <a href="https://jasonppy.github.io/">Puyuan Peng</a>, <a href="https://scholar.google.com.tw/citations?user=3_3EwZ8AAAAJ&hl=zh-TW">Hsuan-Fu Wang</a>, <a href="https://scholar.google.com/citations?user=ERm5NBkAAAAJ&hl=en">Layne Berry</a>, <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php">Hung-yi Lee</a>, <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>
              </div>
              <div style="margin-top:4px"> 
              <em>ICASSP Workshop on Self-supervision in Audio, Speech and Beyond, 2024</em>
              </div>
              <div style="margin-top:4px"> 
              <em>Course Project of "Deep Learning for Human Language Processing", Spring 2023</em>
              </div>
              <div style="margin-top:4px">
              <a href="https://arxiv.org/abs/2402.05819">paper</a> / <a href="images/pwhubert_icasspw24_poster.pdf">poster</a>
              </div>
              <p></p>
              <p>
                We propose using vision as a surrogate for paired transcripts to enrich the semantic information in self-supervised speech models.
              </p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:25px 0px 0px 0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/text_br.png' width="220">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Zero-shot Text Behavior Retrieval</span>
              <div style="margin-top:4px">
              <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung-906.github.io/">Kuo-Han Hung</a>*, <a href="https://openreview.net/profile?id=~Nai-Xuan_Ye1">Nai-Xuan Ye*</a>, <a href="https://github.com/Gordon119">Shao-Syuan Huang*</a> 
              </div>
              <div style="margin-top:4px">
              <em>Course Project of "Reinforcement Learning", Fall 2023</em>
              </div>
              <div style="margin-top:4px">
              <a href="data/Text_Behavior_Retrieval.pdf">paper</a>
              </div>
              <p></p>
              <p>
                We propose a method for retrieving task-relevant data for imitation learning without requiring expert demonstrations. Our approach leverages text descriptions in combination with a vision-language model to enable zero-shot behavior retrieval.
              </p>
            </td>
          </tr>



        </tbody></table> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Teaching and Service</h2>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

         <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/review.png' width="140">
            </div>
          </td>
          <td width="75%" valign="center">
            <b>Reviewer</b>
            <br>
            <br>
            ICLR 2026, TMLR 2025
            <!-- Teaching Assistant, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php" >EE5100: Introduction to Generative Artificial Intelligence, Spring 2024</a>
            <br>
            <br>
            Teaching Assistant, <a href="https://www.csie.ntu.edu.tw/~htlin/course/ml23spring/" >CSIE5043: Machine Learning, Spring 2023</a> -->
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/ntu_logo.png' width="140">
            </div>
          </td>
            
          <td width="75%" valign="center">
            <b>National Taiwan University</b>
            <br>
            <br>
            Teaching Assistant, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php" >EE5100: Introduction to Generative Artificial Intelligence, Spring 2024</a>
            <br>
            <br>
            Teaching Assistant, <a href="https://www.csie.ntu.edu.tw/~htlin/course/ml23spring/" >CSIE5043: Machine Learning, Spring 2023</a>
          </td>
        </tr>

      </tbody></table>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This template is adapted from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
