<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hung-Chieh Fang</title>

    <meta name="author" content="Hung-Chieh Fang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <style>
      a {
        color: #8B0000; /* soft gray */
        text-decoration: none;
      }
  
      a:hover {
        color: #5A0000; /* black on hover */
        text-decoration: underline;
      }
    </style>
    
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hung-Chieh Fang
                </p>
                <p>I am a senior undergrad majoring in Computer Science at National Taiwan University, where I am fortunate to be advised by Professors <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>, <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung (Vivian) Chen</a> and <a href="https://shaohua0116.github.io/">Shao-Hua Sun</a>. </p>
                <p>Currently, I am a visiting research intern at <a href="https://www.stanford.edu/">Stanford University</a>, working with Prof. <a href="https://dorsa.fyi/">Dorsa Sadigh</a> and <a href="https://amberxie88.github.io/">Amber Xie</a>.</p>
                <p>Previously, I was a visiting student at <a href="https://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>, where I had the privilege of working with Dr. <a href="https://yifeiacc.github.io/">Yifei Zhang</a> and Prof. <a href="https://www.cse.cuhk.edu.hk/people/faculty/irwin-king/">Irwin King</a>.</p>
                <p style="text-align:center">
                  <a href="mailto:hungchieh.fang@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/cv_hcfang.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MpGIrR0AAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/hc-fang/">Github</a> &nbsp;/&nbsp;
                  <a href="https://x.com/hungchiehfang">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/HUNG-CHIEH-FANG/">Linkedin</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/hcfang.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/hcfang.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <!-- <table style="margin-top: -20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr><td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <p></p>
            </td></tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-top: -20px;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <tr>  
              <td style="padding: 15px 20px 5px 20px; width:70%; vertical-align:middle;">
                <strong><span class="newsdate">- [June 2025] </span></strong>
                <em>SSD</em> has been accepted to ICCV 2025. See you in Hawaii!
              </td>
            </tr>
            <tr>  
              <td style="padding: 15px 20px 5px 20px; width:70%; vertical-align:middle;">
                <strong><span class="newsdate">- [May 2025] </span></strong>
                Our paper, <em>"Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation"</em>, has been accepted to ICML 2025. See you in Vancouver!
              </td>
            </tr>
            <tr>  
              <td style="padding: 15px 20px 5px 20px; width:70%; vertical-align:middle;">
                <strong><span class="newsdate">- [May 2025] </span></strong>
                I'm fortunate to visit <a href="https://iliad.stanford.edu/">ILIAD</a> at Stanford, hosted by Prof. <a href="https://dorsa.fyi/">Dorsa Sadigh</a> and <a href="https://amberxie88.github.io/">Amber Xie</a>.
              </td>
            </tr>
            <tr>  
              <td style="padding: 15px 20px 5px 20px; width:70%; vertical-align:middle;">
                <strong><span class="newsdate">- [Jul 2024] </span></strong>
                I'm fortunate to visit <a href="https://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>, hosted by Prof. <a href="https://www.cse.cuhk.edu.hk/people/faculty/irwin-king/">Irwin King</a> and Dr. <a href="https://yifeiacc.github.io/">Yifei Zhang</a>.
              </td>
            </tr> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  <!-- I am interested in robot learning, specifically in making RL and IL scalable for tasks that involve extensive interactions beyond basic manipulation. I am also interested in robust machine learning, where I have worked on representation learning under distribution shift and aggregating non-IID data. -->
                  I am broadly interested in robotics, machine learning and reinforcement learning. My research goal is to develope robots capable of performing complex tasks at or beyond human-level proficiency. I am particularly interested in the following areas:
                  <ul style="padding-left: 1.5rem; line-height: 1.3;">
                    <li>
                    <strong>Adaptation to distribution shift</strong> (e.g., <a href="https://dc-unida.github.io/">dc-unida</a>, <a href="https://hc-fang.github.io/data/ssd.pdf">ssd</a>): For robots to be readily used in the real world, they must have almost perfect success rate (> 99%). This requires the ability to adapt (or safely explore) in environments with distribution shifts. I am interested in algorithms that enable <i>test-time</i> adaptation/exploration.
                    </li>

                    <li>
                      <strong>Dexterous manipulation</strong>: To fully unlock the potential of robots in complex tasks such as in-hand manipulation and tool use, fine-grained control is essential. I am interested in how to scale up RL/IL for these tasks.
                    </li>

                    <!-- <li> -->
                      <!-- <strong>Skill-based learning</strong> (e.g., sof): Many real-world tasks are long-horizon and require hierarchical decision-making. I am interested in developing generalizable skill-based frameworks that enable structured planning and exploration. -->
                    <!-- </li> -->
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/sof.png' width="180">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Learning Skill Abstraction from Action-Free Videos via Optical Flow</span>
              <div style="margin-top:2px">
              <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung-906.github.io/">Kuo-Han Hung</a>*, <a href="https://openreview.net/profile?id=~Chu-Rong_Chen1">Chu-Rong Chen</a>, <a href="https://openreview.net/profile?id=~Po-Jung_Chou1">Po-Jung Chou</a>, <a href="https://yck1130.github.io/">Chun-Kai Yang</a>, <a href="https://pochen-ko.github.io/">Po-Chen Ko</a>, <a href="https://vllab.ee.ntu.edu.tw/ycwang.html">Yu-Chiang Wang</a>, <a href="https://kristery.github.io/">Yueh-Hua Wu</a>, <a href="https://minhungchen.netlify.app/">Min-Hung Chen</a>, <a href="https://shaohua0116.github.io/">Shao-Hua Sun</a>
              </div>
              <div style="margin-top:2px">
              <em>ICML Workshop on Building Physically Plausible World Models, 2025 (Prelim. Version)</em>
              </div>
              <div style="margin-top:2px">
              <a href="https://openreview.net/pdf?id=7Udx16p9Rz">manuscript</a>
              </div>
              <p></p>
              <p>
                
              </p>
            </td>
          </tr> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ssd.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</span>
                <div style="margin-top:2px">
                <strong>Hung-Chieh Fang</strong>, <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>, <a href="https://www.cse.cuhk.edu.hk/people/faculty/irwin-king/">Irwin King</a>, <a href="https://yifeiacc.github.io/">Yifei Zhang</a> 
                </div>
                <div style="margin-top:2px">
                <em>International Conference on Computer Vision (ICCV), 2025</em>
                </div>
                <div style="margin-top:2px">
                <a href="https://arxiv.org/abs/2508.01251">paper</a> / <a href="https://ssd-uniformity.github.io/">website</a> / <a href="https://github.com/hc-fang/ssd">code</a>
                </div>
                <p></p>
                <p>
                  We explore how to improve representation quality (<em>uniformity</em>) under limited and restricted information due to the nature of federated learning (non-IID, features not shared across clients). We improve inter-client uniformity by <em>softly</em> separating client embeddings without disrupting the underlying data structure. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/unida_thumbnail.png' width="175">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="papertitle">Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation</span>
                <div style="margin-top:2px">
                <strong>Hung-Chieh Fang</strong>, <a href="https://openreview.net/profile?id=~Po-Yi_Lu1">Po-Yi Lu</a>, <a href="https://www.csie.ntu.edu.tw/~htlin/">Hsuan-Tien Lin</a>
                </div>
                <div style="margin-top:2px">
                <em>International Conference on Machine Learning (ICML), 2025</em>
                </div>
                <div style="margin-top:2px">
                <a href="https://arxiv.org/abs/2410.11271">paper</a> / <a href="https://dc-unida.github.io/">website</a> / <a href="images/unida_icml25_poster.pdf">poster</a>
                </div>
                <p></p>
                <p>
                  We found that partial alignment loss fails to outperform the simplest baselineâ€”training only on source dataâ€”when there is a source-target label imbalance, due to <em>dimensional collapse</em> in target representations. We address this using de-collapse techniques from self-supervised learning, advancing toward more <em>comprehensive</em> universal domain adaptation.
                </p>
              </td>
            </tr>

           
            <tr>
              <td style="padding:10px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/convadrqa.png' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Open-domain Conversational Question Answering with Historical Answers</span>
                <div style="margin-top:4px">
                <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung-906.github.io/">Kuo-Han Hung</a>*, <a href="https://chaoweihuang.github.io/">Chao-Wei Huang</a>, <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung Chen</a>
                </div>
                <div style="margin-top:4px">
                <em>Asian Chapter of the Association for Computational Linguistics (AACL), 2022</em>
                </div>
                <div style="margin-top:4px">
                <a href="https://arxiv.org/abs/2211.09401">paper</a> / <a href="https://github.com/MiuLab/ConvADR-QA">code</a>
                </div>
                <p></p>
                <p>
                  We propose combining the signal from historical answers with the <em>noise-reduction</em> ability of knowledge distillation to improve information retrieval and question answering.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Projects</h2>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/pwhubert.png' width="180">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model</span>
              <div style="margin-top:4px">
              <strong>Hung-Chieh Fang*</strong>, <a href="https://openreview.net/profile?id=~Nai-Xuan_Ye1">Nai-Xuan Ye*</a>, <a href="https://www.cs.utexas.edu/~yjshih/">Yi-Jen Shih</a>, <a href="https://jasonppy.github.io/">Puyuan Peng</a>, <a href="https://scholar.google.com.tw/citations?user=3_3EwZ8AAAAJ&hl=zh-TW">Hsuan-Fu Wang</a>, <a href="https://scholar.google.com/citations?user=ERm5NBkAAAAJ&hl=en">Layne Berry</a>, <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php">Hung-yi Lee</a>, <a href="https://www.cs.utexas.edu/~harwath/">David Harwath</a>
              </div>
              <div style="margin-top:4px">
              <em>ICASSP Workshop on Self-supervision in Audio, Speech and Beyond, 2024</em>
              </div>
              <div style="margin-top:4px">
              <a href="https://arxiv.org/abs/2402.05819">paper</a> / <a href="images/pwhubert_icasspw24_poster.pdf">poster</a>
              </div>
              <p></p>
              <p>
                We propose using vision as a surrogate for paired transcripts to enrich the semantic information in self-supervised speech models.
              </p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/text_br.png' width="180">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Zero-shot Text Behavior Retrieval</span>
              <div style="margin-top:4px">
              <strong>Hung-Chieh Fang*</strong>, <a href="https://khhung-906.github.io/">Kuo-Han Hung</a>*, <a href="https://openreview.net/profile?id=~Nai-Xuan_Ye1">Nai-Xuan Ye*</a>, <a href="https://github.com/Gordon119">Shao-Syuan Huang*</a> 
              </div>
              <div style="margin-top:4px">
              <em>Course Project of "Reinforcement Learning", Fall 2023</em>
              </div>
              <div style="margin-top:4px">
              <a href="data/Text_Behavior_Retrieval.pdf">paper</a>
              </div>
              <p></p>
              <p>
                We propose a method for retrieving task-relevant data for imitation learning without requiring expert demonstrations. Our approach leverages text descriptions in combination with a vision-language model to enable zero-shot behavior retrieval.
              </p>
            </td>
          </tr>



        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Teaching</h2>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/ntu_logo.png' width="140">
            </div>
          </td>
          <td width="75%" valign="center">
            Teaching Assistant, <a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php" >EE5100: Introduction to Generative Artificial Intelligence, Spring 2024</a>
            <br>
            <br>
            Teaching Assistant, <a href="https://www.csie.ntu.edu.tw/~htlin/course/ml23spring/" >CSIE5043: Machine Learning, Spring 2023</a>
          </td>
        </tr>

      </tbody></table>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This template is adapted from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
